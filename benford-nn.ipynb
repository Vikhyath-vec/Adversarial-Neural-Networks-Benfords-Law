{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# imports\nimport numpy as np\nimport pandas as pd\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport torch\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom copy import deepcopy\nimport math\nfrom torch.nn import init\nfrom tqdm import tqdm\nimport json\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the data\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\nbatch_size = 64\n\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# showing sample images from data\ndef imshow(img):\n    img = img / 2 + 0.5\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\nimshow(torchvision.utils.make_grid(images))\nprint(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code to calculate leading digit distribution and subsequently calculating MLH\nbenford = [math.log10(1 + 1 / i) for i in range(1, 10)]\nbenford = [np.round(i, 4) for i in benford]\nbenford_th = torch.FloatTensor(benford)\n\ndef non_bias(m):\n    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear) or isinstance(m, torch.nn.BatchNorm2d):\n        return m.weight\n    return None\n\ndef bincount(tensor):\n    counts = torch.zeros(10)\n    for i in range(10):\n        counts[i] = torch.count_nonzero(tensor == i)\n    return counts\n\n@torch.no_grad()\ndef bin_percent(tensor):\n    tensor = tensor.abs() * 1e10\n    tensor = tensor // 10 ** torch.log10(tensor).long()\n    tensor = bincount(tensor.long())\n    return tensor\n\ndef block_bincount(model):\n    bins = torch.zeros(10)\n    total_params = 0\n    for m in model.modules():\n        if list(m.children()) == []:\n            weights = non_bias(m)\n            if weights is not None:\n                total_params += weights.numel()\n                tensor = bin_percent(weights.view(-1).detach())\n                for i in range(10):\n                    bins[i] += tensor[i]\n    return bins / bins.sum()\n\ndef calculate_mlh(bin_percents):\n    return pearsonr(benford_th, bin_percents[1:])[0]\n\ndef benford_comparison(model):\n    bins = block_bincount(deepcopy(model))\n    return calculate_mlh(bins)\n\n# using Xavier Uniform initialization method to initialize weights for given model\ndef init_params(model):\n    for m in model.modules():\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n            init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                init.constant_(m.bias, 0)\n        elif isinstance(m, torch.nn.BatchNorm2d):\n            init.constant_(m.weight, 1)\n            init.constant_(m.bias, 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# normal training epoch. If optimizer is None, then can be used for validation and test data\ndef epoch(model, criterion, loader, optimizer=None):\n    total_loss = 0.\n    total_err = 0.\n    for X, y in loader:\n        X, y = X.to(device), y.to(device)\n        y_hat = model(X)\n        loss = criterion(y_hat, y)\n        if optimizer:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_err += (y_hat.max(dim=1)[1] != y).sum().item()\n        total_loss += loss.item() * X.shape[0]\n    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n\n# implementing PGD attack under l_infinity norm\ndef pgd_linf(model, X, y, epsilon=0.1, alpha=0.01, num_iter=20, randomize=False):\n    if randomize:\n        delta = torch.rand_like(X, requires_grad=True)\n        delta.data = delta.data * 2 * epsilon - epsilon\n    else:\n        delta = torch.zeros_like(X, requires_grad=True)\n    \n    for t in range(num_iter):\n        loss = F.cross_entropy(model(X + delta), y)\n        loss.backward()\n        delta.data = (delta + alpha * delta.grad.detach().sign()).clamp(-epsilon, epsilon)\n        delta.grad.zero_()\n    return delta.detach()\n\n# adversarial training epoch. If optimizer is None, then can be used for validation and test data\ndef epoch_adv(model, criterion, loader, optimizer=None, **kwargs):\n    total_loss = 0.\n    total_err = 0.\n    for X, y in loader:\n        X, y = X.to(device), y.to(device)\n        delta = pgd_linf(model, X, y, **kwargs)\n        y_hat = model(X + delta)\n        loss = criterion(y_hat, y)\n        if optimizer:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        total_err += (y_hat.max(dim=1)[1] != y).sum().item()\n        total_loss += loss.item() * X.shape[0]\n    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n\n# calculating bounds\ndef bounds_propagation(model, initial_bound):\n    l, u = initial_bound\n    bounds = []\n    for m in model.modules():\n        if isinstance(m, torch.nn.Linear):\n            l_ = (m.weight.clamp(min=0) @ l.t() + m.weight.clamp(max=0) @ u.t() + m.bias[:, None]).t()\n            u_ = (m.weight.clamp(min=0) @ u.t() + m.weight.clamp(max=0) @ l.t() + m.bias[:, None]).t()\n        elif isinstance(m, torch.nn.Conv2d):\n            l_ = F.conv2d(l, m.weight.clamp(min=0), bias=None, stride=m.stride, padding=m.padding, dilation=m.dilation, groups=m.groups) + F.conv2d(u, m.weight.clamp(max=0), bias=None, stride=m.stride, padding=m.padding, dilation=m.dilation, groups=m.groups) + m.bias[None, :, None, None]\n            u_ = F.conv2d(u, m.weight.clamp(min=0), bias=None, stride=m.stride, padding=m.padding, dilation=m.dilation, groups=m.groups) + F.conv2d(l, m.weight.clamp(max=0), bias=None, stride=m.stride, padding=m.padding, dilation=m.dilation, groups=m.groups) + m.bias[None, :, None, None]\n        elif isinstance(m, torch.nn.ReLU):\n            l_ = l.clamp(min=0)\n            u_ = u.clamp(min=0)\n        \n        bounds.append((l_, u_))\n        l, u = l_, u_\n    return bounds\n\ndef interval_based_bound(model, c, bounds, idx):\n    cW = c.t() @ list(model.modules())[-1].weight\n    cb = c.t() @ list(model.modules())[-1].bias\n    \n    l,u = bounds[-2]\n    return (cW.clamp(min=0) @ l[idx].t() + cW.clamp(max=0) @ u[idx].t() + cb[:,None]).t()  \n\n# upper-bound based adversarial training epoch. If optimizer is None, then can be used for validation and test data \ndef epoch_robust_bound(model, criterion, loader, epsilon, optimizer=None):\n    total_loss = 0.\n    total_err = 0.\n\n    C = [-torch.eye(10).to(device) for _ in range(10)]\n    for i in range(10):\n        C[i][i, :] += 1\n\n    for X, y in loader:\n        X, y = X.to(device), y.to(device)\n        initial_bound = (X - epsilon, X + epsilon)\n        bounds = bounds_propagation(model, initial_bound)\n        loss = 0\n        for y0 in range(10):\n            if sum(y == y0) > 0:\n                lower_bound = interval_based_bound(model, C[y0], bounds, y == y0)\n                loss += nn.CrossEntropyLoss(reduction='sum')(-lower_bound, y[y == y0]) / X.shape[0]\n                total_err += (lower_bound.min(dim=1)[0] < 0).sum().item()\n        total_loss += loss.item() * X.shape[0]\n        if optimizer:\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    return total_err / len(loader.dataset), total_loss / len(loader.dataset)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initializing models\nclean_model = models.resnet50().to(device)\ninit_params(clean_model)\npgd_model = models.resnet50().to(device)\npgd_model.load_state_dict(clean_model.state_dict())\nbound_model = models.resnet50().to(device)\nbound_model.load_state_dict(clean_model.state_dict())\n\nclean_mlh_init = benford_comparison(clean_model)\npgd_mlh_init = benford_comparison(pgd_model)\nbound_mlh_init = benford_comparison(bound_model)\nprint(clean_mlh_init, pgd_mlh_init, bound_mlh_init)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# regular training\noptimizer = optim.Adam(clean_model.parameters(), lr=0.003)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50000)\ncriterion = nn.CrossEntropyLoss()\n\nclean_training_accuracy = []\nclean_validation_accuracy = []\nclean_adv_accuracy = []\nclean_mlh_list = []\n\nfor t in tqdm(range(50)):\n    train_err, train_loss = epoch(clean_model, criterion, trainloader, optimizer)\n    test_err, test_loss = epoch(clean_model, criterion, testloader)\n    adv_err, adv_loss = epoch_adv(clean_model, criterion, testloader, epsilon=0.1, alpha=0.05, num_iter=40, randomize=True)\n    scheduler.step()\n    mlh = benford_comparison(clean_model)\n    clean_training_accuracy.append(train_err)\n    clean_validation_accuracy.append(test_err)\n    clean_adv_accuracy.append(adv_err)\n    clean_mlh_list.append(mlh)\n\nprint(clean_training_accuracy)\nprint(clean_validation_accuracy)\nprint(clean_adv_accuracy)\nprint(clean_mlh_list)\nclean_dict = {\n    'clean_training_accuracy': clean_training_accuracy,\n    'clean_validation_accuracy': clean_validation_accuracy,\n    'clean_adv_accuracy': clean_adv_accuracy,\n    'clean_mlh_list': clean_mlh_list\n}\nwith open (\"clean_resnet50.json\", \"w\") as f:\n    json.dump(clean_dict, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adversarial training using PGD adversarial examples\noptimizer = optim.Adam(pgd_model.parameters(), lr=0.003)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50000)\ncriterion = nn.CrossEntropyLoss()\n\npgd_training_accuracy = []\npgd_validation_accuracy = []\npgd_adv_accuracy = []\npgd_mlh_list = []\n\niteration = 0\nfor t in tqdm(range(50)):\n    train_err, train_loss = epoch_adv(pgd_model, criterion, trainloader, optimizer)\n    test_err, test_loss = epoch(pgd_model, criterion, testloader)\n    adv_err, adv_loss = epoch_adv(pgd_model, criterion, testloader)\n    scheduler.step()\n    mlh = benford_comparison(pgd_model)\n    pgd_training_accuracy.append(train_err)\n    pgd_validation_accuracy.append(test_err)\n    pgd_adv_accuracy.append(adv_err)\n    pgd_mlh_list.append(mlh)\n    if iteration % 10 == 9:\n        print(iteration)\n        print(pgd_training_accuracy)\n        print(pgd_validation_accuracy)\n        print(pgd_adv_accuracy)\n        print(pgd_mlh_list)\n    iteration += 1\n    \nprint(\"final\")\nprint(pgd_training_accuracy)\nprint(pgd_validation_accuracy)\nprint(pgd_adv_accuracy)\nprint(pgd_mlh_list)\npgd_dict = {\n    'pgd_training_accuracy': pgd_training_accuracy,\n    'pgd_validation_accuracy': pgd_validation_accuracy,\n    'pgd_adv_accuracy': pgd_adv_accuracy,\n    'pgd_mlh_list': pgd_mlh_list\n}\nwith open (\"pgd_resnet50.json\", \"w\") as f:\n    json.dump(pgd_dict, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adversarial training using upper-bound method\noptimizer = optim.Adam(bound_model.parameters(), lr=0.003)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50000)\ncriterion = nn.CrossEntropyLoss()\n\nbound_training_accuracy = []\nbound_validation_accuracy = []\nbound_adv_accuracy = []\nbound_mlh_list = []\n\niteration = 0\nfor t in tqdm(range(50)):\n    train_err, train_loss = epoch_robust_bound(bound_model, criterion, trainloader, 0.01, optimizer)\n    test_err, test_loss = epoch(bound_model, criterion, testloader)\n    adv_err, adv_loss = epoch_robust_bound(bound_model, criterion, testloader, 0.1)\n    scheduler.step()\n    mlh = benford_comparison(bound_model)\n    bound_training_accuracy.append(train_err)\n    bound_validation_accuracy.append(test_err)\n    bound_adv_accuracy.append(adv_err)\n    bound_mlh_list.append(mlh)\n    if iteration % 10 == 9:\n        print(iteration)\n        print(bound_training_accuracy)\n        print(bound_validation_accuracy)\n        print(bound_adv_accuracy)\n        print(bound_mlh_list)\n    iteration += 1\n\nprint(\"final\")\nprint(bound_training_accuracy)\nprint(bound_validation_accuracy)\nprint(bound_adv_accuracy)\nprint(bound_mlh_list)\nbound_dict = {\n    'bound_training_accuracy': bound_training_accuracy,\n    'bound_validation_accuracy': bound_validation_accuracy,\n    'bound_adv_accuracy': bound_adv_accuracy,\n    'bound_mlh_list': bound_mlh_list\n}\nwith open (\"bound_resnet50.json\", \"w\") as f:\n    json.dump(bound_dict, f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# early stopping using MLH as criteria\nclean_model = models.resnet50().to(device)\ninit_params(clean_model)\n\noptimizer = optim.Adam(clean_model.parameters(), lr=0.003)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50000)\ncriterion = nn.CrossEntropyLoss()\n\nfor t in tqdm(range(60)):\n    train_err, train_loss = epoch(clean_model, criterion, trainloader, optimizer)\n    scheduler.step()\n    mlh = benford_comparison(clean_model)\n    if mlh > 0.998:\n        break\n\ntest_err, test_loss = epoch(clean_model, criterion, testloader)\nmlh_final = benford_comparison(clean_model)\nprint(test_err, test_loss, mlh_final)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation loader\ntrainset_size = len(trainset)\ntrainset_indices = list(range(trainset_size))\nnp.random.shuffle(trainset_indices)\nsplit = int(np.floor(0.2 * trainset_size))\ntrain_idx, valid_idx = trainset_indices[split:], trainset_indices[:split]\ntrain_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\nvalid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=train_sampler)\nvalidloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, sampler=valid_sampler)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# early stopping using validation accuracy as criteria\noptimizer = optim.Adam(clean_model.parameters(), lr=0.003)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50000)\ncriterion = nn.CrossEntropyLoss()\n\nfor t in tqdm(range(60)):\n    train_err, train_loss = epoch(clean_model, criterion, trainloader, optimizer)\n    valid_err, valid_loss = epoch(clean_model, criterion, validloader)\n    scheduler.step()\n    mlh = benford_comparison(clean_model)\n    print(train_err, valid_err, mlh)\n    if valid_err < 0.05:\n        break\n\ntest_err, test_loss = epoch(clean_model, criterion, testloader)\nmlh_final = benford_comparison(clean_model)\nprint(test_err, test_loss,Â mlh_final)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using various initialization methods to initialize weights for given model\ndef multi_init_params(model, initializer):\n    for m in model.modules():\n        if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n            initializer(m.weight)\n            if m.bias is not None:\n                init.constant_(m.bias, 0)\n        elif isinstance(m, torch.nn.BatchNorm2d):\n            init.constant_(m.weight, 1)\n            init.constant_(m.bias, 0)\n\n# initalizing various models and initializers\ntorch_models = [models.resnet18(), models.resnet34(), models.resnet50(), models.resnet101(), models.resnet152(), models.densenet121(), models.densenet161(), models.densenet169(), models.densenet201(), models.inception_v3(), models.googlenet(), models.vgg11(), models.vgg11_bn(), models.vgg13(), models.vgg13_bn(), models.vgg16(), models.vgg16_bn(), models.vgg19(), models.vgg19_bn(), models.mobilenet_v2()]\ntorch_models = [model.to(device) for model in torch_models]\nprint(\"{} Models loaded\".format(len(torch_models)))\ninitializers = [init.kaiming_uniform_, init.kaiming_normal_, init.xavier_uniform_, init.xavier_normal_, init.orthogonal_, init.normal_, init.uniform_, init.trunc_normal_]\nprint(\"{} Initializers loaded\".format(len(initializers)))\nprint(\"Number of models: \", len(torch_models))\nprint(\"Number of initializers: \", len(initializers))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating mlh and bins for every model for every initializer\ninit_mlh_values = np.zeros((len(torch_models), len(initializers)))\ninit_bins = np.zeros((len(torch_models), len(initializers), 9))\nfor i, model in enumerate(torch_models):\n    for j, initializer in enumerate(initializers):\n        print(\"Model: \", i, \" Initializer: \", j)\n        multi_init_params(model, initializer)\n        init_mlh_values[i, j], init_bins[i, j] = benford_comparison(model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# box plot for each initializer\nplt.figure(figsize=(15, 6))\ncolors = sns.color_palette(\"husl\", len(initializers))\nplt.boxplot(init_mlh_values, showmeans=True, meanline=True, meanprops={\"linestyle\": \"--\", \"color\": \"red\"})\nplt.xticks(range(1, len(initializers) + 1), ['Kaiming Uniform', 'Kaiming Normal', 'Xavier Uniform', 'Xavier Normal', 'Orthogonal', 'Normal', 'Uniform', 'Truncated Normal'])\nplt.xlabel(\"Initializer\")\nplt.ylabel(\"MLH\")\nplt.title(\"MLH values for different initializers\")\nplt.grid(alpha=0.3)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# bar plot for benford distribution vs Kaiming Normal initialized weights distribution\ncolors = sns.color_palette(\"husl\", len(torch_models) + 1)\nplt.bar(range(1, 10), benford_th, color=colors[0], label=\"Benford Distribution\")\nfor i in range(len(torch_models)):\n    plt.plot(range(1, 10), init_bins[i, 1], color=colors[i + 1], label=\"Model {}\".format(torch_models[i].__class__.__name__))\nplt.xlabel(\"Digit\")\nplt.xticks(range(1, 10))\nplt.ylabel(\"Frequency\")\nplt.title(\"Benford Distribution vs Uniform initialized weights distribution\")\nplt.grid(alpha=0.3)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}